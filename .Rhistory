clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("\\n", " ", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", " ", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("[^0-9 a-z A-Z ///]", "", clean_tweet)
clean_tweet = tolower(clean_tweet)
# Customize
clean_tweet = gsub("sandiaga uno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiaga", "sandiagauno", clean_tweet)
clean_tweet = gsub("susi pudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("susi", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("anies baswedan", "aniesbaswedan", clean_tweet)
clean_tweet = clean_tweet[clean_tweet!='']
stopw = as_tibble(read.table("data/stopword-id.txt", header = T, stringsAsFactors = F))
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- tidy_text %>% count(word, sort = TRUE)
wordcloud2(tidy_txt, minSize = 1)
clean_tweet = gsub("&amp;", "", tweets$text)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("#\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("\\n", " ", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", " ", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("[^0-9 a-z A-Z ///]", "", clean_tweet)
clean_tweet = tolower(clean_tweet)
# Customize
clean_tweet = gsub("sandiaga uno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiagauno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiaga ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("susi pudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("susi", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("anies baswedan", "aniesbaswedan", clean_tweet)
clean_tweet = clean_tweet[clean_tweet!='']
stopw = as_tibble(read.table("data/stopword-id.txt", header = T, stringsAsFactors = F))
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- tidy_text %>% count(word, sort = TRUE)
wordcloud2(tidy_txt, minSize = 1)
clean_tweet = gsub("&amp;", "", tweets$text)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("#\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("\\n", " ", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", " ", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("[^0-9 a-z A-Z ///]", "", clean_tweet)
clean_tweet = tolower(clean_tweet)
# Customize
clean_tweet = gsub("sandiaga uno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiagauno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiaga ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("susi pudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("susipudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("susi ", "susipudjiastuti ", clean_tweet)
clean_tweet = gsub("anies baswedan", "aniesbaswedan", clean_tweet)
clean_tweet = clean_tweet[clean_tweet!='']
stopw = as_tibble(read.table("data/stopword-id.txt", header = T, stringsAsFactors = F))
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- tidy_text %>% count(word, sort = TRUE)
wordcloud2(tidy_txt, minSize = 1)
clean_tweet = gsub("&amp;", "", tweets$text)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("#\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("\\n", " ", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", " ", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("[^0-9 a-z A-Z ///]", "", clean_tweet)
clean_tweet = tolower(clean_tweet)
# Customize
clean_tweet = gsub("sandiaga uno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiagauno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiaga ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("sandi ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("susi pudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("susipudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("susi ", "susipudjiastuti ", clean_tweet)
clean_tweet = gsub("anies baswedan", "aniesbaswedan", clean_tweet)
clean_tweet = clean_tweet[clean_tweet!='']
stopw = as_tibble(read.table("data/stopword-id.txt", header = T, stringsAsFactors = F))
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- tidy_text %>% count(word, sort = TRUE)
wordcloud2(tidy_txt, minSize = 1)
getUser("sandiagauno")
clean_tweet = gsub("&amp;", "", tweets$text)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("#\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("\\n", " ", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", " ", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("[^0-9 a-z A-Z ///]", "", clean_tweet)
clean_tweet = tolower(clean_tweet)
# Customize
clean_tweet = gsub("sandiaga uno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiagauno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiaga ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("sandi ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("susi ", "susipudjiastuti ", clean_tweet)
clean_tweet = gsub("susi pudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("susipudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("anies baswedan", "aniesbaswedan", clean_tweet)
clean_tweet = clean_tweet[clean_tweet!='']
stopw = as_tibble(read.table("data/stopword-id.txt", header = T, stringsAsFactors = F))
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- tidy_text %>% count(word, sort = TRUE)
wordcloud2(tidy_txt, minSize = 1)
getUser("sandiagauno")
write.table(tweets, "data/tweets.txt", row.names = F)
clean_tweet = gsub("anies ", "aniesbaswedan ", clean_tweet)
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- tidy_text %>% count(word, sort = TRUE)
wordcloud2(tidy_txt, minSize = 1)
View(tidy_text)
tweets$text[9]
tweets$text[8]
clean_tweet = gsub("&amp;", "", tweets$text)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("#\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("\\n", " ", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", " ", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("[^0-9 a-z A-Z ///]", "", clean_tweet)
clean_tweet = tolower(clean_tweet)
clean_tweet = gsub("sandiaga uno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiagauno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiaga ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("sandi ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("susi ", "susipudjiastuti ", clean_tweet)
clean_tweet = gsub("susi pudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("susipudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("anies baswedan", "aniesbaswedan", clean_tweet)
clean_tweet = gsub("anies ", "aniesbaswedan ", clean_tweet)
wordcloud2(tidy_txt, minSize = 1)
wordcloud2
is(tidy_txt)
table(tidy_text)
tidy_txt <- table(tidy_text$word) %>% count(word, sort = TRUE)
table(tidy_text$word)
tidy_txt <- table(tidy_text$word)
wordcloud2(tidy_txt, minSize = 1)
wordcloud2(tidy_txt, minSize = 1)
wordcloud2(tidy_txt, minSize = 1)
wordcloud2(tidy_txt, minSize = 1)
shiny::runApp()
runApp()
runApp()
runApp(port = 5050)
library(shiny)
# library(dplyr)
library(caret)
library(randomForest)
library(doSNOW)
library(ggplot2)
# library(RSQLite)
library(ROAuth)
library(twitteR)
library(stringr)
library(wordcloud2)
library(readr)
library(dplyr)
library(tidytext)
tweets <- twListToDF(searchTwitteR("#rindu", n = 5000))
tweets <- tweets[tweets$isRetweet==FALSE, ]
source("TwitterAuth.R")
twitter.connect()
tweets <- twListToDF(searchTwitteR("#rindu", n = 5000))
tweets <- tweets[tweets$isRetweet==FALSE, ]
tweets <- twListToDF(searchTwitteR("#rindu", n = 5000))
tweets <- tweets[tweets$isRetweet==FALSE, ]
clean_tweet = gsub("&amp;", "", tweets$text)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("#\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("\\n", " ", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", " ", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("[^0-9 a-z A-Z ///]", "", clean_tweet)
clean_tweet = tolower(clean_tweet)
clean_tweet = clean_tweet[clean_tweet!='']
stopw = as_tibble(read.table("data/stopword-id.txt", header = T, stringsAsFactors = F))
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- table(tidy_text$word)
wordcloud2(tidy_txt, minSize = 1)
stopw = as_tibble(read.table("data/stopword-id.txt", header = T, stringsAsFactors = F))
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- table(tidy_text$word)
wordcloud2(tidy_txt, minSize = 1)
wordcloud2(tidy_txt, minSize = 1)
wordcloud2(tidy_txt, minSize = 5)
summary(tidy_txt)
summary(data.frame(tidy_txt))
wordcloud2(tidy_txt, minSize = 2)
tweets <- twListToDF(searchTwitteR("#rindu", n = 5000, lang = "id"))
tweets <- tweets[tweets$isRetweet==FALSE, ]
clean_tweet = gsub("&amp;", "", tweets$text)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("#\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("\\n", " ", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", " ", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("[^0-9 a-z A-Z ///]", "", clean_tweet)
clean_tweet = tolower(clean_tweet)
clean_tweet = clean_tweet[clean_tweet!='']
stopw = as_tibble(read.table("data/stopword-id.txt", header = T, stringsAsFactors = F))
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- table(tidy_text$word)
wordcloud2(tidy_txt, minSize = 3)
summary(data.frame(tidy_txt))
wordcloud2(tidy_txt, minSize = 5)
wordcloud2(tidy_txt, minSize = 5)
wordcloud2(tidy_txt, minSize = 5, shape = "pentagon")
wordcloud2(tidy_txt, minSize = 5, shape = "star")
wordcloud2(tidy_txt, minSize = 5, shape = "cardioid")
wordcloud2(tidy_txt, minSize = 5, shape = "cardioid")
wordcloud2(tidy_txt, minSize = 1, shape = "cardioid")
wordcloud2(tidy_txt, minSize = 1, shape = "cardioid")
a <- data.frame(tidy_txt)
a[a$Var1=="dilan",]
devtools::install_github("hadley/r4ds")
install.packages("data.table")
library("data.table", lib.loc="~/R/win-library/3.4")
address(1)
address2
address(2)
# Create
friends_data <- data_frame(
name = c("Nicolas", "Thierry", "Bernard", "Jerome"),
age = c(27, 25, 29, 26),
height = c(180, 170, 185, 169),
married = c(TRUE, FALSE, TRUE, TRUE)
)
# Print
friends_data
library("tidyr", lib.loc="~/R/win-library/3.4")
# Create
friends_data <- data_frame(
name = c("Nicolas", "Thierry", "Bernard", "Jerome"),
age = c(27, 25, 29, 26),
height = c(180, 170, 185, 169),
married = c(TRUE, FALSE, TRUE, TRUE)
)
# Print
friends_data
library("tidyverse", lib.loc="~/R/win-library/3.4")
# Create
friends_data <- data_frame(
name = c("Nicolas", "Thierry", "Bernard", "Jerome"),
age = c(27, 25, 29, 26),
height = c(180, 170, 185, 169),
married = c(TRUE, FALSE, TRUE, TRUE)
)
# Print
friends_data
vignete(tibble)
vignette("tibble")
tibble(a = integer(), b = 1)
tibble(a = 1:3, c = 1:2)
library("caret", lib.loc="~/R/win-library/3.4")
train(Species ~ ., data = iris, method = "rf",
# options to `randomForest`:
importance = TRUE)
mod <- train(Species ~ ., data = iris, method = "rf",
# options to `randomForest`:
importance = TRUE)
library("ggthemes", lib.loc="~/R/win-library/3.4")
library("sentimentr", lib.loc="~/R/win-library/3.4")
install.packages("SentimentAnalysis")
library("SentimentAnalysis", lib.loc="~/R/win-library/3.4")
library(tm)
# via vector of strings
corpus <- c("Positive text", "Neutral but uncertain text", "Negative text")
sentiment <- analyzeSentiment(corpus)
compareToResponse(sentiment, c(+1, 0, -2))
data("crude")
sentiment <- analyzeSentiment(crude)
sentiment
crude
head(crude)
shiny::runApp()
runApp()
runApp()
runApp()
runApp(host="0.0.0.0", port = 5050)
runApp()
runApp()
runApp(host="0.0.0.0", port = 5050)
runApp()
runApp()
tweets <- twListToDF(searchTwitteR("#rindu", n = 5000, lang = "id"))
tweets <- tweets[tweets$isRetweet==FALSE, ]
clean_tweet = gsub("&amp;", "", tweets$text)
clean_tweet = gsub("(RT|via)((?:\\b\\W*@\\w+)+)", "", clean_tweet)
clean_tweet = gsub("@\\w+", "", clean_tweet)
clean_tweet = gsub("#\\w+", "", clean_tweet)
clean_tweet = gsub("[[:punct:]]", "", clean_tweet)
clean_tweet = gsub("[[:digit:]]", "", clean_tweet)
clean_tweet = gsub("http\\w+", "", clean_tweet)
clean_tweet = gsub("\\n", " ", clean_tweet)
clean_tweet = gsub("[ \t]{2,}", " ", clean_tweet)
clean_tweet = gsub("^\\s+|\\s+$", "", clean_tweet)
clean_tweet = gsub("[^0-9 a-z A-Z ///]", "", clean_tweet)
clean_tweet = tolower(clean_tweet)
# Customize
clean_tweet = gsub("sandiaga uno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiagauno", "sandiagauno", clean_tweet)
clean_tweet = gsub("sandiaga ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("sandi ", "sandiagauno ", clean_tweet)
clean_tweet = gsub("susi ", "susipudjiastuti ", clean_tweet)
clean_tweet = gsub("susi pudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("susipudjiastuti", "susipudjiastuti", clean_tweet)
clean_tweet = gsub("anies baswedan", "aniesbaswedan", clean_tweet)
clean_tweet = gsub("anies ", "aniesbaswedan ", clean_tweet)
clean_tweet = clean_tweet[clean_tweet!='']
stopw = as_tibble(read.table("data/stopword-id.txt", header = T, stringsAsFactors = F))
text_df <- data_frame(line = 1:length(clean_tweet), text = clean_tweet)
tidy_text <- text_df %>%
unnest_tokens(word, text) %>%
anti_join(stopw)
tidy_txt <- table(tidy_text$word)
wordcloud(tidy_txt, minSize = 1, shape = "cardioid")
wordcloud::wordcloud(tidy_txt, minSize = 1, shape = "cardioid")
tidy_txt2 <- table(tidy_text$word)
wordcloud2(tidy_txt2, minSize = 1, shape = "cardioid")
tidy_txt <- as.data.frame(tidy_txt2)
wordcloud::wordcloud(tidy_txt)
wordcloud::wordcloud(words = tidy_txt$Var1, freq = tidy_txt$Freq)
library(shiny)
library(wordcloud2)
shinyApp(
ui=shinyUI(fluidPage(
#using default clicked word input id
wordcloud2Output("my_wc"),
#using custom clicked word input id
wordcloud2Output("my_wc2", clickedWordInputId = "wc2_clicked_word"),
verbatimTextOutput("print"),
verbatimTextOutput("print2")
)),
server=shinyServer(function(input,output,session){
output$my_wc  = renderWordcloud2(wordcloud2(demoFreq))
output$my_wc2 = renderWordcloud2(wordcloud2(demoFreq))
#using default clicked word input id
output$print  = renderPrint(input$my_wc_clicked)
#using custom clicked word input id
output$print2 = renderPrint(input$wc2_clicked_word)
})
)
library(shiny)
library(wordcloud2)
shinyApp(
ui=shinyUI(fluidPage(
#using default clicked word input id
wordcloud2Output("my_wc"),
verbatimTextOutput("print"),
verbatimTextOutput("print2")
)),
server=shinyServer(function(input,output,session){
output$my_wc  = renderWordcloud2(wordcloud2(demoFreq))
output$my_wc2 = renderWordcloud2(wordcloud2(demoFreq))
#using default clicked word input id
output$print  = renderPrint(input$my_wc_clicked)
})
)
runApp()
runApp()
tidy_text1 <- sapply(tidy_text$word, katadasaR)
library(katadasaR)
tidy_text1 <- sapply(tidy_text$word, katadasaR)
tidy_txt <- table(tidy_text1$word)
tidy_txt <- table(tidy_text1$Var1)
tidy_txt <- table(tidy_text1)
tidy_text1 <- sapply(as.character(tidy_text$word), katadasaR)
wordcloud2(tidy_txt)
runApp()
?woeid
?weoid
availableTrendLocations()
woeid = availableTrendLocations[1, "woeid"]
t1 <- getTrends(woeid)
woeid = availableTrendLocations()[1, "woeid"]
t1 <- getTrends(woeid)
View(t1)
woeid = availableTrendLocations()
t1 <- getTrends(woeid)
woeid = availableTrendLocations()
View(woeid)
woeid[woeid$woeid %in% c(1030077, 23424846),]
woeid[woeid$woeid %in% c(1030077, 1032539, 1040779, 1044316, 1046138, 1047180, 1047378, 1047908, 1048059, 1048324, 1048536, 23424846),]
woeid[woeid$country %in% 'Indonesia',]
woeid <- data.frame(woeid = c(23424846, 1030077, 1032539, 1040779, 1044316, 1046138, 1047180, 1047378, 1047908, 1048059, 1048324, 1048536),
name = c("Indonesia", "Bekasi", "Depok", "Pekanbaru", "Surabaya", "Makassar", "Bandung", "Jakarta", "Medan", "Palembang", "Semarang", "Tangerang"))
runApp()
woeid <- availableTrendLocations()
View(woeid)
runApp()
runApp()
runApp()
runApp()
woeid <- availableTrendLocations()
runApp()
runApp()
rm(woeid)
runApp()
runApp()
rm(woeid)
runApp()
woeid <- availableTrendLocations()
write.csv(woeid, "data/woeid.csv", row.names = F)
woeid <- read_csv("data/woeid.csv")
rm(woeid)
runApp()
woeid <- read_csv("data/woeid.csv")
View(woeid)
runApp()
rm(woeid)
runApp()
runApp()
runApp()
runApp()
woeid <- read_csv("data/woeid.csv")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
rm(woeid)
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
?isolate
runApp()
library(rvest)
webpage <- read_html("https://www.nytimes.com/interactive/2017/06/23/opinion/trumps-lies.html")
webpage
results <- webpage %>% html_nodes(".short-desc")
results
first_result <- results[1]
first_result %>% html_nodes("strong")
library(stringr)
str_c(date, ', 2017')
runApp()
library(RCurl)
webpage <- getURL("https://www.federalreserve.gov/newsevents/press/monetary/20160427a.htm")
webpage
webpage <- getURL("https://www.federalreserve.gov/newsevents/press/monetary/20160427a.htm", ssl.verifypeer = FALSE)
webpage
library(XML)
Apr2016.tree <- htmlTreeParse(Apr216.request, useInternal = TRUE)
Apr2016.tree <- htmlTreeParse(webpage, useInternal = TRUE)
Apr2016.tree
Apr2016.tree.parse <- unlist(xpathApply(Apr2016.tree, path = "//p", fun = xmlValue))
print(Apr2016.tree.parse)
webpage <- getURL("https://www.federalreserve.gov/newsevents/pressreleases/bcreg20180301a.htm", ssl.verifypeer = FALSE)
webpage
Apr2016.tree <- htmlTreeParse(webpage, useInternal = TRUE)
Apr2016.tree.parse <- unlist(xpathApply(Apr2016.tree, path = "//p", fun = xmlValue))
print(Apr2016.tree.parse)
